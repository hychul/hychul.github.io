# 특징

1. 온메모리
2. 분산 병렬처리
   => 때문에 하둡보다 100배 정도 빠르다

# Apache Spark Process

```
+-----------+  +------------------------+  +--------------------+  +--------------------+
| Spark SQL |  |    Spark Streaming     |  |        MLib        |  |       GraphX       |
|  & Shark  |  | (real-time processing) |  | (machine learning) |  | (graph processing) |  :  Component
+-----------+  +------------------------+  +--------------------+  +--------------------+
+---------------------------------------------------------------------------------------+
|                                       Spark Core                                      |  :  Core
+---------------------------------------------------------------------------------------+
+---------------------------------------+  +--------------------+  +--------------------+
|          Standalone Scheduler         |  |        YARN        |  |        Mesos       |  :  Resource Manager
+---------------------------------------+  +--------------------+  +--------------------+
```

Standalone : 한대에서 사용할때
YARN : 어러대의 클러스에서 사용할떄

# 클러스터

```
                                                        +--------------+
+-----------------+                                     | Worker Node  |
| Driver Program  |         +-----------------+         | - Executor   |
| - Spark Context | <-----> | Cluster Manager | <-----> |   - Task ... |
+-----------------+         +-----------------+         | - Cache      |
                                                        +--------------+
                                                               ⋮
       Master                Resource Manager                Slave
```

클러스터는 JVM 위에서 동작한다

## Standalone

Master와 Slave가 하나의 JVM위에서 스레드로 구성되어 동작한다.

## Fully Distributes Mode

물리적인 서버로 각 노드가 분리되어 동작한다.

# Spark Distribution vs Haddop Map-reduce

https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1
https://stackoverflow.com/questions/37737110/how-is-task-distributed-in-spark

# Spark shuffle is not good

Spark join create data suffling

# Spark Join

https://sparkbyexamples.com/spark/spark-sql-dataframe-join/
